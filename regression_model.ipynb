{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd0d4f083a7fa1d632f3ea86866ddf8c4c5ce9bed1c588ac8dc3a0713b496b78046",
   "display_name": "Python 3.8.10 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "d4f083a7fa1d632f3ea86866ddf8c4c5ce9bed1c588ac8dc3a0713b496b78046"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 머신러닝 회귀 모델"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리와 데이터 로드\n",
    "\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql.functions import udf, col \n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Setting for visualization\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "\n",
    "from matplotlib import rcParams\n",
    "sns.set(context='notebook', style='whitegrid',\n",
    "        rc={'figure.figsize': (18, 4)})\n",
    "rcParams['figure.figsize'] = 18, 4\n",
    "\n",
    "# 노트북 재실행을 대비하기 위해 랜덤 시드 설정\n",
    "rnd_seed = 23\n",
    "np.random.seed = rnd_seed\n",
    "np.random.set_state = rnd_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(long=-122.2300033569336, lat=37.880001068115234, medage=41.0, totrooms=880.0, totbdrms=129.0, pop=322.0, houshlds=126.0, medinc=8.325200080871582, medhv=452600.0),\n",
       " Row(long=-122.22000122070312, lat=37.86000061035156, medage=21.0, totrooms=7099.0, totbdrms=1106.0, pop=2401.0, houshlds=1138.0, medinc=8.301400184631348, medhv=358500.0),\n",
       " Row(long=-122.23999786376953, lat=37.849998474121094, medage=52.0, totrooms=1467.0, totbdrms=190.0, pop=496.0, houshlds=177.0, medinc=7.257400035858154, medhv=352100.0),\n",
       " Row(long=-122.25, lat=37.849998474121094, medage=52.0, totrooms=1274.0, totbdrms=235.0, pop=558.0, houshlds=219.0, medinc=5.643099784851074, medhv=341300.0),\n",
       " Row(long=-122.25, lat=37.849998474121094, medage=52.0, totrooms=1627.0, totbdrms=280.0, pop=565.0, houshlds=259.0, medinc=3.8461999893188477, medhv=342200.0)]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data_path = './data/cal_housing.data'\n",
    "# data의 컬럼명 스키마로 명시\n",
    "schema_string = 'long lat medage totrooms totbdrms pop houshlds medinc medhv'\n",
    "\n",
    "fields = [StructField(field, FloatType(), True) for field in schema_string.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# 데이터 로드를 위한 SparkSession 만들기\n",
    "spark = SparkSession.builder.master('local[2]')\\\n",
    "        .appName('Linear-Regression-California-housing')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# 데이터 로드\n",
    "housing_df = spark.read.csv(path=data_path, schema=schema).cache()\n",
    "\n",
    "# take()는 데이터 미리보기를 Python named tuple 형태인 DataFrame의 Row 형식으로 보여준다.\n",
    "housing_df.take(5)"
   ]
  },
  {
   "source": [
    "# 데이터 탐색"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+\n",
      "|medage|count|\n",
      "+------+-----+\n",
      "|  52.0| 1273|\n",
      "|  51.0|   48|\n",
      "|  50.0|  136|\n",
      "|  49.0|  134|\n",
      "|  48.0|  177|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|               long|              lat|            medage|          totrooms|         totbdrms|               pop|         houshlds|            medinc|             medhv|\n",
      "+-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|              20640|            20640|             20640|             20640|            20640|             20640|            20640|             20640|             20640|\n",
      "|   mean|-119.56970444871473|35.63186143109965|28.639486434108527|2635.7630813953488|537.8980135658915|1425.4767441860465|499.5396802325581|3.8706710030346416|206855.81690891474|\n",
      "| stddev|  2.003531742932898|2.135952380602968| 12.58555761211163|2181.6152515827944| 421.247905943133|  1132.46212176534|382.3297528316098|1.8998217183639696|115395.61587441359|\n",
      "|    min|            -124.35|            32.54|               1.0|               2.0|              1.0|               3.0|              1.0|            0.4999|           14999.0|\n",
      "|    max|            -114.31|            41.95|              52.0|           39320.0|           6445.0|           35682.0|           6082.0|           15.0001|          500001.0|\n",
      "+-------+-------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 그룹핑해서 집계\n",
    "result_df = housing_df.groupBy('medage').count()\n",
    "# 값 정렬 기준 설정해주고 내림차순으로 정렬\n",
    "result_df.sort('medage', ascending=False).show(5)\n",
    "\n",
    "# 수치형 변수들 기술통계량으로 살펴보기\n",
    "housing_df.describe().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-------+---------+--------+---------+--------+-------+-----------+\n|summary| medage| totrooms|totbdrms|      pop|houshlds| medinc|      medhv|\n+-------+-------+---------+--------+---------+--------+-------+-----------+\n|  count|20640.0|  20640.0| 20640.0|  20640.0| 20640.0|20640.0|    20640.0|\n|   mean|28.6395|2635.7631| 537.898|1425.4767|499.5397| 3.8707|206855.8169|\n| stddev|12.5856|2181.6153|421.2479|1132.4621|382.3298| 1.8998|115395.6159|\n|    min|    1.0|      2.0|     1.0|      3.0|     1.0| 0.4999|    14999.0|\n|    max|   52.0|  39320.0|  6445.0|  35682.0|  6082.0|15.0001|   500001.0|\n+-------+-------+---------+--------+---------+--------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 반환되는 통계량 값들을 소수점 제거하거나 하는 등 커스터마이징해서 출력\n",
    "(housing_df.describe()).select('summary',\n",
    "                              F.round('medage', 4).alias('medage'),\n",
    "                              F.round('totrooms', 4).alias('totrooms'),\n",
    "                              F.round('totbdrms', 4).alias('totbdrms'),\n",
    "                              F.round('pop', 4).alias('pop'),\n",
    "                              F.round('houshlds', 4).alias('houshlds'),\n",
    "                              F.round('medinc', 4).alias('medinc'),\n",
    "                              F.round('medhv', 4).alias('medhv')).show(10)"
   ]
  },
  {
   "source": [
    "# Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-----+------+--------+--------+------+--------+------+--------+--------+--------+----------+\n|   long|  lat|medage|totrooms|totbdrms|   pop|houshlds|medinc|   medhv|rmsperhh|popperhh|bdrmsperrm|\n+-------+-----+------+--------+--------+------+--------+------+--------+--------+--------+----------+\n|-122.23|37.88|  41.0|   880.0|   129.0| 322.0|   126.0|8.3252|452600.0|    6.98|    2.56|      0.15|\n|-122.22|37.86|  21.0|  7099.0|  1106.0|2401.0|  1138.0|8.3014|358500.0|    6.24|    2.11|      0.16|\n|-122.24|37.85|  52.0|  1467.0|   190.0| 496.0|   177.0|7.2574|352100.0|    8.29|     2.8|      0.13|\n|-122.25|37.85|  52.0|  1274.0|   235.0| 558.0|   219.0|5.6431|341300.0|    5.82|    2.55|      0.18|\n|-122.25|37.85|  52.0|  1627.0|   280.0| 565.0|   259.0|3.8462|342200.0|    6.28|    2.18|      0.17|\n+-------+-----+------+--------+--------+------+--------+------+--------+--------+--------+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 파생변수 생성\n",
    "housing_df = (housing_df.withColumn('rmsperhh',\n",
    "                                    F.round(col('totrooms')/col('houshlds'), 2))\\\n",
    "                        .withColumn('popperhh',\n",
    "                                    F.round(col('pop')/col('houshlds'), 2))\\\n",
    "                        .withColumn('bdrmsperrm',\n",
    "                                    F.round(col('totbdrms')/col('totrooms'), 2)))\n",
    "\n",
    "housing_df.show(5)"
   ]
  },
  {
   "source": [
    "이제 필요한 변수들만 골라서 데이터프레임을 새로 할당한 후 수치형 변수들에 Scaling을 적용시켜보자. 특이한 점은 Scaling을 적용시키기 이전에 필요한 Feature들을 Vector로 변환하는 작업이 선행되어야 한다는 것이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+--------+------+--------+------+--------+--------+----------+--------------------+\n|   medhv|totbdrms|   pop|houshlds|medinc|rmsperhh|popperhh|bdrmsperrm|            features|\n+--------+--------+------+--------+------+--------+--------+----------+--------------------+\n|452600.0|   129.0| 322.0|   126.0|8.3252|    6.98|    2.56|      0.15|[129.0,322.0,126....|\n|358500.0|  1106.0|2401.0|  1138.0|8.3014|    6.24|    2.11|      0.16|[1106.0,2401.0,11...|\n|352100.0|   190.0| 496.0|   177.0|7.2574|    8.29|     2.8|      0.13|[190.0,496.0,177....|\n|341300.0|   235.0| 558.0|   219.0|5.6431|    5.82|    2.55|      0.18|[235.0,558.0,219....|\n|342200.0|   280.0| 565.0|   259.0|3.8462|    6.28|    2.18|      0.17|[280.0,565.0,259....|\n|269700.0|   213.0| 413.0|   193.0|4.0368|    4.76|    2.14|      0.23|[213.0,413.0,193....|\n|299200.0|   489.0|1094.0|   514.0|3.6591|    4.93|    2.13|      0.19|[489.0,1094.0,514...|\n|241400.0|   687.0|1157.0|   647.0|  3.12|     4.8|    1.79|      0.22|[687.0,1157.0,647...|\n|226700.0|   665.0|1206.0|   595.0|2.0804|    4.29|    2.03|      0.26|[665.0,1206.0,595...|\n|261100.0|   707.0|1551.0|   714.0|3.6912|    4.97|    2.17|       0.2|[707.0,1551.0,714...|\n+--------+--------+------+--------+------+--------+--------+----------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 사용하지 않을 변수 제외하고 필요한 변수들만 select\n",
    "housing_df = housing_df.select('medhv',\n",
    "                              'totbdrms',\n",
    "                              'pop',\n",
    "                              'houshlds',\n",
    "                              'medinc',\n",
    "                              'rmsperhh',\n",
    "                              'popperhh',\n",
    "                              'bdrmsperrm')\n",
    "\n",
    "featureCols = ['totbdrms', 'pop', 'houshlds', 'medinc',\n",
    "                'rmsperhh', 'popperhh', 'bdrmsperrm']\n",
    "\n",
    "# VectorAssembler로 feature vector로 변환\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol='features')\n",
    "assembled_df = assembler.transform(housing_df)\n",
    "assembled_df.show(10, truncate=True)"
   ]
  },
  {
   "source": [
    "Scaling 하는 API도 PySpark에서 제공하는데 여러가지 스케일링 방법이 있지만 여기서는 표준화인 StandardScaler를 사용해보자. 메소드 사용 방법은 다음과 같다. StandardScaler(inputCol='Raw Features', outputCol='Scaled Features') 로 스케일링을 정의해주고 fit(dataframe)과 transform(dataframe)으로 스케일링을 수행해주자."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------------------+\n|            features|     features_scaled|\n+--------------------+--------------------+\n|[129.0,322.0,126....|[0.30623297630686...|\n|[1106.0,2401.0,11...|[2.62553233949916...|\n|[190.0,496.0,177....|[0.45104081781631...|\n|[235.0,558.0,219....|[0.55786627466754...|\n|[280.0,565.0,259....|[0.66469173151877...|\n|[213.0,413.0,193....|[0.50564049576249...|\n|[489.0,1094.0,514...|[1.16083663111672...|\n|[687.0,1157.0,647...|[1.63086864126214...|\n|[665.0,1206.0,595...|[1.57864286235709...|\n|[707.0,1551.0,714...|[1.67834662208491...|\n+--------------------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Feature들을 스케일링 시키기\n",
    "\n",
    "standdardScaler = StandardScaler(inputCol='features', outputCol='features_scaled')\n",
    "\n",
    "scaled_df = standdardScaler.fit(assembled_df).transform(assembled_df)\n",
    "\n",
    "# scaling된 피쳐들만 추출해보기\n",
    "scaled_df.select('features', 'features_scaled').show(10, truncate=True)"
   ]
  },
  {
   "source": [
    "# 데이터 분할과 회귀 모델링\n",
    "\n",
    "회귀 모델은 여러가지가 있지만 여기서는 오버피팅을 예방하기 위한 정규화 항이 포함되어 있는 Elastic Net 회귀 모델사용해보자"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "train_data, test_data = scaled_df.randomSplit([0.8, 0.2], seed=rnd_seed)\n",
    "\n",
    "# Elastic Net\n",
    "lr = LinearRegression(featuresCol='features_scaled',\n",
    "                      labelCol='medhv',\n",
    "                      predictionCol='predmedhv',\n",
    "                      maxIter=10,\n",
    "                      regParam=0.3,\n",
    "                      elasticNetParam=0.8,\n",
    "                      standardization=False)\n",
    "\n",
    "# model training\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "source": [
    "# 테스트 데이터에 대한 예측"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "predictions = linearModel.transform(test_data)\n",
    "\n",
    "print(type(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------+-------+\n|         predmedhv|  medhv|\n+------------------+-------+\n|  65820.1760042106|14999.0|\n|221374.23211624727|22500.0|\n| 40972.55542160492|22500.0|\n|111770.06878754942|26900.0|\n| 176224.2459104087|34400.0|\n| 80414.73100645898|36700.0|\n|145622.16563780906|37500.0|\n| 78208.77719512783|39400.0|\n|136096.33496158314|39800.0|\n|108725.69135250017|40900.0|\n| 76631.25587698547|41700.0|\n| 75815.91817725686|42500.0|\n|101987.67841479904|42500.0|\n| 80076.98310851591|43000.0|\n| 76251.18681418494|43600.0|\n|108256.88381314362|44000.0|\n| 93202.79491312054|44000.0|\n| 92720.30368070511|44400.0|\n| 66433.07339323626|44500.0|\n|141201.75137895203|44600.0|\n+------------------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# y값과 예측값만 추출해서 비교해보기\n",
    "pred_labels = predictions.select('predmedhv', 'medhv')\n",
    "pred_labels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE:78116.2564798975\nMAE: 56925.44456037988\nR2 score: 0.5435224404301482\n"
     ]
    }
   ],
   "source": [
    "# 그렇다면 예측값과 실제값이 얼마나 차이나는지 RMSE, MAE를 계산해보고 만들어진 회귀식이 주어진 데이터를 얼마나 잘 설명하는지 나타내는 R2 Score(결정계수)값을 살펴보자.\n",
    "\n",
    "print(f\"RMSE:{linearModel.summary.rootMeanSquaredError}\")\n",
    "print(f\"MAE: {linearModel.summary.meanAbsoluteError}\")\n",
    "print(f\"R2 score: {linearModel.summary.r2}\")"
   ]
  },
  {
   "source": [
    "추가적으로 메트릭을 얻을 수 있는 방법은 위 코드 말고도 RegressionEvaluator로도 계산할 수 있다. 해당 메소드의 사용방법은 다음과 같다.\n",
    "\n",
    "- RegressionEvaluator(predictionCol, labelCol, metricName)\n",
    "    - predictionCol : 예측값\n",
    "    - labelCol : 실제값\n",
    "    - metricName : 측정할 메트릭 이름(ex. RMSE, MAE 등등)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE: 0.5595706743501381\n",
      "MAE: 0.5595706743501381\n",
      "R2 score: 0.5595706743501381\n"
     ]
    }
   ],
   "source": [
    "# RMSE\n",
    "evaluator = RegressionEvaluator(predictionCol='predmedhv',\n",
    "                               labelCol='medhv',\n",
    "                               metricName='rmse')\n",
    "# MAE                            \n",
    "evaluator = RegressionEvaluator(predictionCol='predmedhv',\n",
    "                               labelCol='medhv',\n",
    "                               metricName='mae')\n",
    "# R2 Score\n",
    "evaluator = RegressionEvaluator(predictionCol='predmedhv',\n",
    "                               labelCol='medhv',\n",
    "                               metricName='r2')\n",
    "\n",
    "print(f\"RMSE: {evaluator.evaluate(pred_labels)}\")\n",
    "print(f\"MAE: {evaluator.evaluate(pred_labels)}\")  \n",
    "print(f\"R2 score: {evaluator.evaluate(pred_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE: 75954.86368997344\nMAE: 55516.495481755104\nR2 score: 0.5595706743501381\n"
     ]
    }
   ],
   "source": [
    "# 또 다른 방법은 RegressionMetrics 메소드인데 이는 특이하게 예측값을 RDD 자료구조로 변환하고 넣어주어야 한다.\n",
    "# 예측값인 pred_labels를 RDD 자료구조로 변환하고 넣어야 함\n",
    "metrics = RegressionMetrics(pred_labels.rdd)\n",
    "\n",
    "print(\"RMSE:\", metrics.rootMeanSquaredError)\n",
    "print(\"MAE:\", metrics.meanAbsoluteError)\n",
    "print(\"R2 score:\", metrics.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}