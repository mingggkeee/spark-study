{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd0d4f083a7fa1d632f3ea86866ddf8c4c5ce9bed1c588ac8dc3a0713b496b78046",
   "display_name": "Python 3.8.10 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "d4f083a7fa1d632f3ea86866ddf8c4c5ce9bed1c588ac8dc3a0713b496b78046"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('Python Spark SQL basic example')\\\n",
    "        .config('spark.some.config.option', 'some-value')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "### Create json file using spark\n",
    "# SparkContext로 객체 생성\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_from_file = sc.\\\n",
    "    textFile(\n",
    "        './data/VS14MORT.txt.gz',\n",
    "        4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractInformation(row):\n",
    "    import re\n",
    "    import numpy as np\n",
    "\n",
    "    selected_indices = [\n",
    "         2,4,5,6,7,9,10,11,12,13,14,15,16,17,18,\n",
    "         19,21,22,23,24,25,27,28,29,30,32,33,34,\n",
    "         36,37,38,39,40,41,42,43,44,45,46,47,48,\n",
    "         49,50,51,52,53,54,55,56,58,60,61,62,63,\n",
    "         64,65,66,67,68,69,70,71,72,73,74,75,76,\n",
    "         77,78,79,81,82,83,84,85,87,89\n",
    "    ]\n",
    "\n",
    "    '''\n",
    "        Input record schema\n",
    "        schema: n-m (o) -- xxx\n",
    "            n - position from\n",
    "            m - position to\n",
    "            o - number of characters\n",
    "            xxx - description\n",
    "        1. 1-19 (19) -- reserved positions\n",
    "        2. 20 (1) -- resident status\n",
    "        3. 21-60 (40) -- reserved positions\n",
    "        4. 61-62 (2) -- education code (1989 revision)\n",
    "        5. 63 (1) -- education code (2003 revision)\n",
    "        6. 64 (1) -- education reporting flag\n",
    "        7. 65-66 (2) -- month of death\n",
    "        8. 67-68 (2) -- reserved positions\n",
    "        9. 69 (1) -- sex\n",
    "        10. 70 (1) -- age: 1-years, 2-months, 4-days, 5-hours, 6-minutes, 9-not stated\n",
    "        11. 71-73 (3) -- number of units (years, months etc)\n",
    "        12. 74 (1) -- age substitution flag (if the age reported in positions 70-74 is calculated using dates of birth and death)\n",
    "        13. 75-76 (2) -- age recoded into 52 categories\n",
    "        14. 77-78 (2) -- age recoded into 27 categories\n",
    "        15. 79-80 (2) -- age recoded into 12 categories\n",
    "        16. 81-82 (2) -- infant age recoded into 22 categories\n",
    "        17. 83 (1) -- place of death\n",
    "        18. 84 (1) -- marital status\n",
    "        19. 85 (1) -- day of the week of death\n",
    "        20. 86-101 (16) -- reserved positions\n",
    "        21. 102-105 (4) -- current year\n",
    "        22. 106 (1) -- injury at work\n",
    "        23. 107 (1) -- manner of death\n",
    "        24. 108 (1) -- manner of disposition\n",
    "        25. 109 (1) -- autopsy\n",
    "        26. 110-143 (34) -- reserved positions\n",
    "        27. 144 (1) -- activity code\n",
    "        28. 145 (1) -- place of injury\n",
    "        29. 146-149 (4) -- ICD code\n",
    "        30. 150-152 (3) -- 358 cause recode\n",
    "        31. 153 (1) -- reserved position\n",
    "        32. 154-156 (3) -- 113 cause recode\n",
    "        33. 157-159 (3) -- 130 infant cause recode\n",
    "        34. 160-161 (2) -- 39 cause recode\n",
    "        35. 162 (1) -- reserved position\n",
    "        36. 163-164 (2) -- number of entity-axis conditions\n",
    "        37-56. 165-304 (140) -- list of up to 20 conditions\n",
    "        57. 305-340 (36) -- reserved positions\n",
    "        58. 341-342 (2) -- number of record axis conditions\n",
    "        59. 343 (1) -- reserved position\n",
    "        60-79. 344-443 (100) -- record axis conditions\n",
    "        80. 444 (1) -- reserve position\n",
    "        81. 445-446 (2) -- race\n",
    "        82. 447 (1) -- bridged race flag\n",
    "        83. 448 (1) -- race imputation flag\n",
    "        84. 449 (1) -- race recode (3 categories)\n",
    "        85. 450 (1) -- race recode (5 categories)\n",
    "        86. 461-483 (33) -- reserved positions\n",
    "        87. 484-486 (3) -- Hispanic origin\n",
    "        88. 487 (1) -- reserved\n",
    "        89. 488 (1) -- Hispanic origin/race recode\n",
    "     '''\n",
    "\n",
    "    record_split = re\\\n",
    "        .compile(\n",
    "            r'([\\s]{19})([0-9]{1})([\\s]{40})([0-9\\s]{2})([0-9\\s]{1})([0-9]{1})([0-9]{2})' + \n",
    "            r'([\\s]{2})([FM]{1})([0-9]{1})([0-9]{3})([0-9\\s]{1})([0-9]{2})([0-9]{2})' + \n",
    "            r'([0-9]{2})([0-9\\s]{2})([0-9]{1})([SMWDU]{1})([0-9]{1})([\\s]{16})([0-9]{4})' +\n",
    "            r'([YNU]{1})([0-9\\s]{1})([BCOU]{1})([YNU]{1})([\\s]{34})([0-9\\s]{1})([0-9\\s]{1})' +\n",
    "            r'([A-Z0-9\\s]{4})([0-9]{3})([\\s]{1})([0-9\\s]{3})([0-9\\s]{3})([0-9\\s]{2})([\\s]{1})' + \n",
    "            r'([0-9\\s]{2})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([\\s]{36})([A-Z0-9\\s]{2})([\\s]{1})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([\\s]{1})([0-9\\s]{2})([0-9\\s]{1})' + \n",
    "            r'([0-9\\s]{1})([0-9\\s]{1})([0-9\\s]{1})([\\s]{33})([0-9\\s]{3})([0-9\\s]{1})([0-9\\s]{1})')\n",
    "    try:\n",
    "        rs = np.array(record_split.split(row))[selected_indices]\n",
    "    except:\n",
    "        rs = np.array(['-99'] * len(selected_indices))\n",
    "    return rs\n",
    "#     return record_split.split(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_file_conv = data_from_file.map(extractInformation)"
   ]
  },
  {
   "source": [
    "# 스키마\n",
    "\n",
    "데이터프레임과 달리 RDD는 스키마리스<sup>schema-less</sup> 데이터 구조다. 그러므로 RDD를 사용할 때 데이터셋을 병렬 처리하는 것은 전혀 문제가 되지 않는다.\n",
    "그래서 튜플, 사전, 리스트와 같은 거의 모든 데이터 구조를 섞을 수 있고, 이는 스파크에서 전혀 문제가 되지 않는다.<br>\n",
    "데이터셋에 대해 .collect() 함수를 수행하면 파이썬에서 일반적으로 했던 방식대로 객체 내의 데이터에 접근 할 수 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_heterogenous = sc.parallelize([\n",
    "    ('Ferrari', 'fast'),\n",
    "    {'Porsche': 100000},\n",
    "    ['Spain', 'visited', 4504]\n",
    "]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data_heterogenous[1]['Porsche']"
   ]
  },
  {
   "source": [
    "## map() 트랜스포메이션"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2014 = data_from_file_conv.map(lambda row: int(row[16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, -99]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "data_2014.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('2014', 2014),\n",
       " ('2014', 2014),\n",
       " ('2014', 2014),\n",
       " ('2014', 2014),\n",
       " ('2014', 2014),\n",
       " ('2014', 2014),\n",
       " ('2014', 2014),\n",
       " ('2014', 2014),\n",
       " ('2014', 2014),\n",
       " ('-99', -99)]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "data_2014_2 = data_from_file_conv.map(\n",
    "    lambda row: (row[16], int(row[16])))\n",
    "data_2014_2.take(10)"
   ]
  },
  {
   "source": [
    "## filter() 트랜스포메이션\n",
    "\n",
    "data_from_file_conv 데이터셋에서 2014년에 사고로 몇 명이 죽었는지를 카운트"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "data_filtered = data_from_file_conv.filter(\n",
    "    lambda row: row[16] == '2014' and row[21] == '0')\n",
    "data_filtered.count()"
   ]
  },
  {
   "source": [
    "## flatmap(...) 트랜스포메이션\n",
    "map() 함수와 비슷하게 동작. 그러나 이는 리스트가 아닌 평면화된 결과를 리턴한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['2014', 2015, '2014', 2015, '2014', 2015, '2014', 2015, '2014', 2015]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "data_2014_flat = data_from_file_conv.flatMap(lambda row: (row[16], int(row[16]) + 1))\n",
    "data_2014_flat.take(10)"
   ]
  },
  {
   "source": [
    "## distinct() 트랜스포메이션\n",
    "특정 칼럼에서의 중복된 값을 제거해, 고유한 값을 리스트로 리턴."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['M', 'F', '-99']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "distinct_gender = data_from_file_conv.map(\n",
    "    lambda row: row[5]).distinct()\n",
    "distinct_gender.collect()"
   ]
  },
  {
   "source": [
    "## sample() 트랜스포메이션\n",
    " sample() 트랜스포메이션\n",
    "데이터셋으로부터 임의로 추출된 샘플을 리턴한다. 첫 번째 파라미터는 중복 허용 여부 명시, 두 번째 파라미터는 리턴할 데이터셋과 전체 데이터셋 간의 크기 비율을 명시, 세 번째 파라미터는 임의의 숫자를 생성하기 위한 시드"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 0.1\n",
    "data_sample = data_from_file_conv.sample(False, fraction, 666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original dataset: 2631171, sample: 263247\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset: {0}, sample: {1}'.format(data_from_file_conv.count(), data_sample.count()))"
   ]
  },
  {
   "source": [
    "## leftOuterJoin() 트랜스포메이션\n",
    " leftOuterJoin() 트랜스포메이션\n",
    "SQL에서처럼 두 개의 RDD를 두 개의 데이터셋에서 찾은 값에 기반에 조인하고, 두 개의 RDD가 매치되는 데이터에 대해 왼쪽 RDD에 오른쪽 RDD가 추가된 결과가 리턴"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([('a', 1), ('b', 4), ('c', 10)])\n",
    "rdd2 = sc.parallelize([('a', 4), ('a', 1), ('b', 6), ('d', 15)])\n",
    "rdd3 = rdd1.leftOuterJoin(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('b', (4, 6)), ('c', (10, None)), ('a', (1, 4)), ('a', (1, 1))]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('b', (4, 6)), ('a', (1, 4)), ('a', (1, 1))]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "rdd4 = rdd1.join(rdd2)\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('a', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "rdd5 = rdd1.intersection(rdd2)\n",
    "rdd5.collect()"
   ]
  },
  {
   "source": [
    "## repartion() 트랜스포메이션"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "rdd1 = rdd1.repartition(4)\n",
    "len(rdd1.glom().collect())"
   ]
  },
  {
   "source": [
    "# 액션\n",
    "트랜스포메이션과는 다르게, 액션은 데이터셋에서 스케줄된 테스크를 실행한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## take() 함수\n",
    "map() 함수처럼 유용한 함수. 이 함수는 하나의 파티션에서 가장 위에 있는 n행을 리턴하기 때문에 RDD 전체를 리턴하는 collect()보다 더 많이 쓰인다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## reduce() 함수\n",
    "특정 함수를 사용해 RDD의 개수를 줄인다. <br>\n",
    "RDD의 총합을 구하기 위해 이 함수를 사용할 수 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "rdd1.map(lambda row: row[1]).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "source": [
    "## count() 함수\n",
    "RDD의 엘리먼트 개수를 센다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## saveAsTextFile() 함수\n",
    " saveAsTextFile() 함수\n",
    "RDD를 텍스트 파일로 저장"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## foreach() 함수\n",
    "RDD의 각 엘리먼트에 반복적으로 적용하는 함수 map() 함수와는 달리, foreach() 함수는 정의된 함수를 하나하나 각각의 데이터에 적용한다. 이는 Pyspark에서 지원하지 않는 데이터베이스에 데이터를 저장하고 싶을 때 유용하다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}