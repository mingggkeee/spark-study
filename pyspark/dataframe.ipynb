{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd0d4f083a7fa1d632f3ea86866ddf8c4c5ce9bed1c588ac8dc3a0713b496b78046",
   "display_name": "Python 3.8.10 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "d4f083a7fa1d632f3ea86866ddf8c4c5ce9bed1c588ac8dc3a0713b496b78046"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('Python Spark SQL basic example')\\\n",
    "        .config('spark.some.config.option', 'some-value')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "### Create json file using spark\n",
    "# SparkContext로 객체 생성\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "source": [
    "## 데이터프레임 생성하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### JSON 데이터 생성하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringJSONRDD = sc.parallelize((\n",
    "    \"\"\"{\"id\": \"123\", \"name\": \"Katie\", \"age\": 19, \"eyeColor\": \"brown\"}\"\"\",\n",
    "    \"\"\"{\"id\": \"234\", \"name\": \"Michael\", \"age\": 22, \"eyeColor\": \"green\"}\"\"\",\n",
    "    \"\"\"{\"id\": \"345\", \"name\": \"Simone\", \"age\": 23, \"eyeColor\": \"blue\"}\"\"\"\n",
    "))"
   ]
  },
  {
   "source": [
    "### 데이터프레임 생성하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmersJSON = spark.read.json(stringJSONRDD)"
   ]
  },
  {
   "source": [
    "### 임시테이블 생성하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
   ]
  },
  {
   "source": [
    "많은 RDD 함수는 액션 함수가 실행되기 이전까지는 실행되지 않은 트랜스포메이션이다. <br>\n",
    "예를 들어, 다음 코드에서 sc.parallelize는 spark.read.json을 사용해 RDD를 데이터프레임으로 변환할 때 실행되는 트랜스포메이션이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 간단한 데이터프레임 쿼리"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 데이터프레임 API 쿼리"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+--------+---+-------+\n|age|eyeColor| id|   name|\n+---+--------+---+-------+\n| 19|   brown|123|  Katie|\n| 22|   green|234|Michael|\n| 23|    blue|345| Simone|\n+---+--------+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "swimmersJSON.show()"
   ]
  },
  {
   "source": [
    "### SQL 쿼리"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "spark.sql(\"select * from swimmersJSON\").collect()"
   ]
  },
  {
   "source": [
    "collect() 함수나 show() 함수는 데이터프레임과 SQL 쿼리에 대해 사용할 수 있다. 단, collect() 함수는 데이터프레임의 모든 행을 리턴하고 실행 노드에서 드라이버 노드로 이동하기 때문에 작은 데이터프레임에 대해 사용하는 것이 좋다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- age: long (nullable = true)\n |-- eyeColor: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# 스키마 출력하기\n",
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "source": [
    "## 프로그래밍하는 것처럼 스키마 명시하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types import\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# 콤마로 분리된 데이터 생성\n",
    "stringCSVRDD = sc.parallelize([\n",
    "    (123, 'Katie', 19, 'brown'),\n",
    "    (234, 'Micheal', 22, 'green'),\n",
    "    (345, 'Simone', 23, 'blue')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스키마를 명시한다.\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD에 스키마를 적용하고 데이터프레임을 생성\n",
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)\n",
    "\n",
    "# 데이터프레임을 이용해 임시 뷰를 생성\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- eyeColor: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "swimmers.printSchema()"
   ]
  },
  {
   "source": [
    "## 데이터프레임 API로 쿼리하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 행의 개수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "swimmers.count()"
   ]
  },
  {
   "source": [
    "### 필터문 실행하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# age가 22인 데이터의 id와 age를 출력\n",
    "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()\n",
    "\n",
    "# 위의 코드를 작성하는 다른 방법\n",
    "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+--------+\n|  name|eyeColor|\n+------+--------+\n| Katie|   brown|\n|Simone|    blue|\n+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# eyeColor가 b로 시작하는 데이터의 name, eyeColor 컬럼을 출력\n",
    "swimmers.select('name', 'eyeColor').filter(\"eyeColor like 'b%'\").show()"
   ]
  },
  {
   "source": [
    "## SQL로 쿼리하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 행의 개수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|       3|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from swimmers\").show()"
   ]
  },
  {
   "source": [
    "### 필터문을 where 절을 사용해 실행하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---+\n| id|age|\n+---+---+\n|234| 22|\n+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "# age가 22인 데이터의 id와 age를 출력\n",
    "spark.sql(\"select id, age from swimmers where age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+--------+\n|  name|eyeColor|\n+------+--------+\n| Katie|   brown|\n|Simone|    blue|\n+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# eyeColor가 b로 시작하는 데이터의 name, eyeColor 컬럼을 출력\n",
    "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"
   ]
  },
  {
   "source": [
    "## 데이터프레임 시나리오: 비행 기록 성능\n",
    "데이터프레임으로 쿼리할 수 있는 타입을 보여주기 위해 비행 기록 성능에 대한 유스케이스를 살펴보자. 항공사의 지연율과 비행 지연의 원인에 대해 분석해보자. 또한 비행 지연의 여러 변수들을 살펴보기 위해 공항 데이터셋과 조인해보자. 이를 통해 비행 지연과 관련된 변수들을 더 잘 이해할 수 있을 것이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 출발지 데이터셋 준비하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# path\n",
    "flightPerFilePath = \"./data/departuredelays.csv\"\n",
    "airportsFilePath = \"./data/airport-codes-na.txt\"\n",
    "\n",
    "# dataset load\n",
    "airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "# datasest load\n",
    "flightPerf = spark.read.csv(flightPerFilePath, header='true')\n",
    "flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n",
    "\n",
    "# 출발지 지연 데이터셋 캐시 (바로 다음 쿼리가 더 빨리 수행되도록)\n",
    "flightPerf.cache()"
   ]
  },
  {
   "source": [
    "### 비행 성능 데이터셋과 공항 데이터셋 조인하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+------+--------+\n|   City|origin|  Delays|\n+-------+------+--------+\n|Seattle|   SEA|159086.0|\n|Spokane|   GEG| 12404.0|\n|  Pasco|   PSC|   949.0|\n+-------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 도시와 출발지 코드에 따라서 비행 지연 합을 쿼리하기(워싱턴 주에 대해)\n",
    "spark.sql(\"\"\"\n",
    "    select a.City,\n",
    "    f.origin,\n",
    "    sum(f.delay) as Delays\n",
    "    from FlightPerformance f\n",
    "    join airports a\n",
    "    on a.IATA = f.origin\n",
    "    where a.State = \"WA\"\n",
    "    group by a.City, f.origin\n",
    "    order by sum(f.delay) desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+------+--------+\n|   City|origin|  Delays|\n+-------+------+--------+\n|Seattle|   SEA|159086.0|\n|Spokane|   GEG| 12404.0|\n|  Pasco|   PSC|   949.0|\n+-------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 도시와 출발지 코드에 따라 비행 지연 합을 쿼리하기(워싱턴 주에 대해)\n",
    "spark.sql(\"\"\"\n",
    "select a.City, f.origin, sum(f.delay) as Delays\n",
    "from FlightPerformance f\n",
    "    join airports a \n",
    "        on a.IATA = f.origin \n",
    "where a.State = 'WA'\n",
    "group by a.City, f.origin\n",
    "order by sum(f.delay) desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+---------+\n|State|   Delays|\n+-----+---------+\n|   SC|  80666.0|\n|   AZ| 401793.0|\n|   LA| 199136.0|\n|   MN| 256811.0|\n|   NJ| 452791.0|\n|   OR| 109333.0|\n|   VA|  98016.0|\n| null| 397237.0|\n|   RI|  30760.0|\n|   WY|  15365.0|\n|   KY|  61156.0|\n|   NH|  20474.0|\n|   MI| 366486.0|\n|   NV| 474208.0|\n|   WI| 152311.0|\n|   ID|  22932.0|\n|   CA|1891919.0|\n|   CT|  54662.0|\n|   NE|  59376.0|\n|   MT|  19271.0|\n+-----+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select a.State, sum(f.delay) as Delays\n",
    "    from FlightPerformance f\n",
    "        join airports a\n",
    "            on a.IATA = f.origin\n",
    "where a.Country = 'USA'\n",
    "group by a.State\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}