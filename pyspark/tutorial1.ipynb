{
 "cells": [
  {
   "source": [
    "# 가장 먼저 할 일! SparkSession !\n",
    "가장 먼저 해야할 것은 SparkContext라는 스파크 객체를 만들어 주어야 한다. SparkContext를 만들어 주기 위해서 우선 SparkSession을 만들어 주자. 그리고 json 파일 형식으로 되어 있는 데이터를 읽어서 데이터의 스키마를 출력시켜보자."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230fcff0",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('Python Spark SQL basic example')\\\n",
    "        .config('spark.some.config.option', 'some-value')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "### Create json file using spark\n",
    "# SparkContext로 객체 생성\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# json 파일 읽어들이기\n",
    "path = './data/people.json'\n",
    "peopleDF = spark.read.json(path)\n",
    "\n",
    "# printSchema()로 json파일의 스키마 형태 볼 수 있음\n",
    "peopleDF.printSchema()"
   ]
  },
  {
   "source": [
    "# SQL로 DataFrame 형태로 데이터 출력해보기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+\n|  name|\n+------+\n|Justin|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# spark에서 제공하는 sql 메소드를 이용해 쿼리 날리기\n",
    "# 쿼리문에서 people 테이블은 위에서 만들었던 view 테이블임!\n",
    "teenagerNamesDF = spark.sql(\"select name from people where age between 13 and 19\")\n",
    "teenagerNamesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(path)\n",
    "\n",
    "# global temporary view 생성\n",
    "df.createOrReplaceGlobalTempView('people')\n",
    "\n",
    "# 'global_temp' 라는 키워드 꼭 붙이기\n",
    "sqlDF = spark.sql('select * from global_temp.people')\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+----+\n|         address|name|\n+----------------+----+\n|{Columbus, Ohio}| Yin|\n+----------------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# 또한 데이터프레임은 RDD[String] 자료구조를 이용해서 json 데이터셋을 데이터프레임으로 만들 수 있음\n",
    "jsonStrings = ['{\"name\": \"Yin\", \"address\":{\"city\":\"Columbus\", \"state\":\"Ohio\"}}']\n",
    "# json -> RDD형식으로 만들기\n",
    "otherPeopleRDD = sc.parallelize(jsonStrings)\n",
    "# json파일 읽어오기\n",
    "otherPeople = spark.read.json(otherPeopleRDD)\n",
    "otherPeople.show()"
   ]
  },
  {
   "source": [
    "# Spark의 DataFrame의 칼럼에 접근해보자!\n",
    "\n",
    "DataFrame Operations 라고도 하면 Untyped Dataset Operations 라고도 한다. Spark 내부에서 Dataset 과 DataFrame의 차이라고 한다면 Typed/Untyped 차이라고 한다. Typed dataset은 dataset으로 받아오는 데이터의 형태를 미리 정의해 놓은 것인 반면 Untyped dataset은 프로그램이 데이터의 형태를 추측해서 가져오는 것을 의미한다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(path)\n",
    "\n",
    "df.select('name').show()\n",
    "\n",
    "# 추가적으로 2개 이상의 컬럼을 추출하면서 기존의 컬럼에 연산을 가해 파생변수를 생성하여 추출하는 것도 가능하다.\n",
    "df.select(df['name'], df['age']+1).show()\n",
    "\n",
    "# age가 20보다 큰 데이터만 추출\n",
    "df.filter(df['age'] > 20).show()\n",
    "\n",
    "# age 컬럼으로 그룹핑 하고 데이터의 개수를 집계\n",
    "df.groupBy('age').count().show()"
   ]
  },
  {
   "source": [
    "# DataFrame의 Schema를 프로그래밍스럽게 명시해보자!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+---+\n|   name|age|\n+-------+---+\n|Michael| 29|\n|   Andy| 30|\n| Justin| 19|\n+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# SparkContext 객체 생성\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# txt file load\n",
    "lines = sc.textFile('./data/people.txt')\n",
    "parts = lines.map(lambda l: l.split(','))\n",
    "\n",
    "## Step 1 -> value들 처리\n",
    "# 각 라인을 tuple( , ) 형태로 convert\n",
    "people = parts.map(lambda p: (p[0], p[1].strip())) # name에서 공백 strip\n",
    "\n",
    "## Step 2 -> Schema들 처리\n",
    "# 문자열로 인코딩된 스키마\n",
    "schemaString = \"name age\"\n",
    "# schemaString 요소를 loop돌면서 StructField로 만들기\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "# StructField 여러개가 있는 리스트를 StructType로 만들기\n",
    "schema = StructType(fields)\n",
    "\n",
    "## Step 3 -> value와 schema 활용해 Dataframe 생성\n",
    "# 위에서 만든 schema를 RDD의 schema로 적용\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "# View Table 생성해 쿼리 날려서 데이터 추출\n",
    "schemaPeople.createOrReplaceTempView('people')\n",
    "results = spark.sql('select * from people')\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3810jvsc74a57bd0d4f083a7fa1d632f3ea86866ddf8c4c5ce9bed1c588ac8dc3a0713b496b78046",
   "display_name": "Python 3.8.10 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "d4f083a7fa1d632f3ea86866ddf8c4c5ce9bed1c588ac8dc3a0713b496b78046"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}